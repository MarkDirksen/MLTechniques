{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_array, _safe_indexing\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class TransformedTargetClassifier(TransformedTargetRegressor):\n",
    "   \n",
    "    def _fit_transformer(self, y):\n",
    "        \"\"\"Check transformer and fit transformer.\n",
    "        Create the default transformer, fit it and make additional inverse\n",
    "        check on a subset (optional).\n",
    "        \"\"\"\n",
    "        if (self.transformer is not None and\n",
    "                (self.func is not None or self.inverse_func is not None)):\n",
    "            raise ValueError(\"'transformer' and functions 'func'/\"\n",
    "                             \"'inverse_func' cannot both be set.\")\n",
    "        elif self.transformer is not None:\n",
    "            self.transformer_ = clone(self.transformer)\n",
    "        else:\n",
    "            if self.func is not None and self.inverse_func is None:\n",
    "                raise ValueError(\"When 'func' is provided, 'inverse_func' must\"\n",
    "                                 \" also be provided\")\n",
    "            self.transformer_ = FunctionTransformer(\n",
    "                func=self.func, inverse_func=self.inverse_func, validate=True,\n",
    "                check_inverse=self.check_inverse)\n",
    "        # XXX: sample_weight is not currently passed to the\n",
    "        # transformer. However, if transformer starts using sample_weight, the\n",
    "        # code should be modified accordingly. At the time to consider the\n",
    "        # sample_prop feature, it is also a good use case to be considered.\n",
    "        self.transformer_.fit(y)\n",
    "        if self.check_inverse:\n",
    "            idx_selected = slice(None, None, max(1, y.shape[0] // 10))\n",
    "            y_sel = _safe_indexing(y, idx_selected)\n",
    "            y_sel_t = self.transformer_.transform(y_sel)\n",
    "            if not np.allclose(y_sel,\n",
    "                               self.transformer_.inverse_transform(y_sel_t)):\n",
    "                warnings.warn(\"The provided functions or transformer are\"\n",
    "                              \" not strictly inverse of each other. If\"\n",
    "                              \" you are sure you want to proceed regardless\"\n",
    "                              \", set 'check_inverse=False'\", UserWarning)\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        **fit_params : dict\n",
    "            Parameters passed to the ``fit`` method of the underlying\n",
    "            regressor.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        y = check_array(y, accept_sparse=False, force_all_finite=True,ensure_2d=False, dtype=None)\n",
    "\n",
    "        # store the number of dimension of the target to predict an array of\n",
    "        # similar shape at predict\n",
    "        self._training_dim = y.ndim\n",
    "\n",
    "        # transformers are designed to modify X which is 2d dimensional, we\n",
    "        # need to modify y accordingly.\n",
    "        if y.ndim == 1:\n",
    "            y_2d = y.reshape(-1, 1)\n",
    "        else:\n",
    "            y_2d = y\n",
    "        self._fit_transformer(y_2d)\n",
    "\n",
    "        # transform y and convert back to 1d array if needed\n",
    "        y_trans = self.transformer_.transform(y_2d)\n",
    "        # FIXME: a FunctionTransformer can return a 1D array even when validate\n",
    "        # is set to True. Therefore, we need to check the number of dimension\n",
    "        # first.\n",
    "        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n",
    "            y_trans = y_trans.squeeze(axis=1)\n",
    "\n",
    "        if self.regressor is None:\n",
    "            from ..linear_model import LinearRegression\n",
    "            self.regressor_ = LinearRegression()\n",
    "        else:\n",
    "            self.regressor_ = clone(self.regressor)\n",
    "\n",
    "        self.regressor_.fit(X, y_trans, **fit_params)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load dataset for classification\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create training and target data\n",
    "\"\"\"\n",
    "X = iris['data']\n",
    "Y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "\"\"\"\n",
    "create alternate target set where integers are tranformed to strings: 0 = 'a', 1 = 'b' etc.\n",
    "\"\"\"\n",
    "Y_train_string = np.array([chr(97 + i) for i in Y_train])\n",
    "Y_test_string = np.array([chr(97 + i) for i in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:18:34] WARNING: ..\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "valid fit\n",
    "\"\"\"\n",
    "classifier.fit(X_train , Y_train);\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdirksen\\Anaconda3\\envs\\XGBoost\\lib\\site-packages\\numpy\\core\\numeric.py:2327: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-ed9aa7256092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTry\u001b[0m \u001b[0mit\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY_train_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\XGBoost\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1121\u001b[0m                 \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m             ):\n\u001b[1;32m-> 1123\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_encoding_check_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The label must consist of integer labels of form 0, 1, 2, ..., [num_class - 1]."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fails because XGBoost expects integer labels 0,1,...,n-1 when use_label_encoder=False\n",
    "Try it out!\n",
    "\"\"\"\n",
    "classifier.fit(X_train , Y_train_string);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine our XGBoost classifier with a transformer that maps string labels to integers 0,1,...,n-1.\n",
    "We use TransformedTargetClassifier, which inherits from TransformedTargetRegressor.\n",
    "\"\"\"\n",
    "transformer = OrdinalEncoder()\n",
    "transformed_classifier = TransformedTargetClassifier(regressor=classifier,transformer=transformer,check_inverse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:23:14] WARNING: ..\\src\\learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "transformed_classifier.fit(X_train , Y_train_string);\n",
    "predictions_string = transformed_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "perform the same string transformation to the output prediction\n",
    "\"\"\"\n",
    "predictions_transformed = np.array([chr(97 + i) for i in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check whether the direct prediction are identical to the predictions using TransformedTargetClassifier\n",
    "\"\"\"\n",
    "np.all(predictions_string == predictions_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XGBoost",
   "language": "python",
   "name": "xgboost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
